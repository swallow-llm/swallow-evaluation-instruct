{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e614d75",
   "metadata": {},
   "source": [
    "A notebook to test a pair of a provider and a model: \\\n",
    "This implementation is based on \"3.4 特定のproviderでエラーが出る場合の対応\" in `README_t4.md`.\n",
    "\n",
    "Setup: \\\n",
    "To use this notebook, you need to use `python>=3.10.0` and install as follows\n",
    "```\n",
    "pip install \"../../lighteval[math,extended_tasks,litellm,vllm]\" \"transformers>=4.51.0,<4.53.0\" \"openai>=1.40.0\" \"datasets<4.0.0\" \"ipywidgets\"\n",
    "```\n",
    "This lineup can be changed due to an update or your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8509e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"lighteval[math,extended_tasks,litellm,vllm]\" \"transformers>=4.51.0,<4.53.0\" \"openai>=1.40.0\" \"datasets<4.0.0\" \"ipywidgets\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c67854",
   "metadata": {},
   "source": [
    "## Call vllm in litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61836f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert load_dotenv('../../.env'), \"Failed to load .env file\"\n",
    "\n",
    "import litellm\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ffcffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_base_url(provider):\n",
    "    base_url_dict = {\n",
    "        \"openai\": \"https://api.openai.com/v1\",\n",
    "        \"deepinfra\": \"https://api.deepinfra.com/v1/openai\",\n",
    "        \"vllm\": \"http://localhost:10001/v1\",\n",
    "    }\n",
    "    return base_url_dict[provider]\n",
    "\n",
    "def get_api_key(provider):\n",
    "    api_name_dict = {\n",
    "        \"openai\": \"OPENAI_API_KEY\",\n",
    "        \"deepinfra\": \"DEEPINFRA_API_KEY\",\n",
    "        \"vllm\": None,\n",
    "    }\n",
    "    return os.getenv(api_name_dict[provider]) if api_name_dict[provider] else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03544e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "provider = \"vllm\"\n",
    "base_url = get_base_url(provider)\n",
    "\n",
    "model = \"google/gemma-3-4b-it\"\n",
    "\n",
    "optional_params = {\n",
    "    # You should refer to the official documentation for the parameters: https://docs.litellm.ai/docs/api-reference/litellm.completion.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 32768,\n",
    "}\n",
    "\n",
    "api_key = get_api_key(provider)\n",
    "if api_key != \"\": optional_params[\"api_key\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a test prompt\n",
    "test_prompt = \"\"\"こんにちは\"\"\"\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f566dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request payload\n",
    "request_payload = {\n",
    "    \"model\": f\"{provider}/{model}\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": test_prompt,\n",
    "        }\n",
    "    ],\n",
    "    \"logprobs\": None,\n",
    "    \"caching\": False,\n",
    "    \"base_url\": base_url,\n",
    "    **optional_params\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12545bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the payload\n",
    "request_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b659e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get responses (this takes a while)\n",
    "responses = litellm.completion(**request_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943dd53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10d67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(responses.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7eea1e",
   "metadata": {},
   "source": [
    "## Use vllm serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225d9c2",
   "metadata": {},
   "source": [
    "Firstly, serve your model like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d9019",
   "metadata": {},
   "source": [
    "```terminal\n",
    "export CUDA_VISIBLE_DEVICES=0,1\n",
    "uv run --isolated --project /home/saito-k/github/swallow-evaluation-instruct-private --locked --extra vllm \\\n",
    "    vllm serve \"google/gemma-3-4b-it\" \\\n",
    "        --port 10001 \\\n",
    "        --hf-token \"hf_...\" \\\n",
    "        --tensor-parallel-size 2 \\\n",
    "        --max-model-len 32768 \\\n",
    "        --gpu-memory-utilization 0.9 \\\n",
    "        --dtype bfloat16 \\\n",
    "        2>&1 &\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8202a",
   "metadata": {},
   "source": [
    "⚠️: Make sure that\n",
    "- the `CUDA_VISIBLE_DEVICES` and the port do not conflict with other processes\n",
    "- the `hf_token` must be filled with your own hf-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c86833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a client\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"dummy\",\n",
    "    base_url=\"http://localhost:10001/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a test prompt\n",
    "test_prompt = \"\"\"こんにちは\"\"\"\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"google/gemma-3-4b-it\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": test_prompt}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
