# You can add custom settings and name them as you like.
# Following are the available settings:
# - system_message
# - max_model_length
# - max_new_tokens
# - temperature
# - top_p
# - max_n
# - reasoning_parser (Set a proper parser if the model supports reasoning tags. Available parsers: https://docs.vllm.ai/en/stable/features/reasoning_outputs.html)
# - vllm_use_v1 (Set 0 if the model is incompatible with FlashAttention-2. Official documentation: https://docs.vllm.ai/en/stable/configuration/env_vars.html)

# 'version' is the version of the custom settings.
# You should update this value if you make any changes to the settings.

default:
  # This model is incompatible with flashattn v1.
  vllm_use_v1: 0
  version: "1"

