# You can add custom settings and name them as you like.
# Following are the available settings:
# - system_message
# - max_model_length
# - max_new_tokens
# - temperature
# - top_p
# - max_n
# - reasoning_parser (Set a proper parser if the model supports reasoning tags. Available parsers: https://docs.vllm.ai/en/stable/features/reasoning_outputs.html)
# - vllm_use_v1 (This option will be deprecated. Please don't use it. Official documentation: https://docs.vllm.ai/en/stable/configuration/env_vars.html)
# - reasoning_effort: (For the litellm backend, this value is passed to litellm.completion(reasoning_effort=...). For the vLLM backend, it is passed to tokenizer.apply_chat_template(). Unsupported models will either raise an error or ignore this parameter.)

# 'version' is the version of the custom settings.
# You should update this value if you make any changes to the settings.

# If you want to unset a parameter explicitly, set it to 'null'.

default:
  # default settings.
  system_message: "You are a helpful assistant."
  max_model_length: 1024
  max_new_tokens: 512
  temperature: 0.6
  top_p: 0.95
  version: "1"

coding:
  # coding settings.
  system_message: "You are a helpful coding assistant."
  max_n: 4
  temperature: null
  top_p: null
  version: "1"

reasoning:
  # settings for reasoning models.
  system_message: "You are a helpful reasoning assistant."
  max_model_length: 8192
  max_new_tokens: 4096
  temperature: 1.0
  top_p: 0.95
  reasoning_parser: "deepseek-r1"
  version: "1"

