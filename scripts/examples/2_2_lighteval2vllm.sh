#!/bin/bash
set -euo pipefail

export VLLM_WORKER_MULTIPROC_METHOD=spawn
export VLLM_USE_V1=0 # vLLM V0ãƒ¢ãƒ¼ãƒ‰ã‚’æŒ‡å®šã™ã‚‹è¨­å®š

MODEL_NAME="tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5"
TASK_ID="swallow|gpqa:diamond"

MODEL_ARGS="pretrained=$MODEL_NAME,dtype=bfloat16,generation_parameters={temperature:0.0}"
# MODEL_ARGSã«ã¯ dtypeï¼Œtensor_parallel_sizeï¼Œmax_model_lengthï¼Œgpu_memory_utlizationï¼Œãã—ã¦å„ç¨® generation_parameters ãªã©ã‚‚æŒ‡å®šã§ãã‚‹ï¼

cd swallow-evaluation-instruct

# è©•ä¾¡
echo "ğŸ¦ Evaluation has started"
uv run --isolated --locked --extra lighteval \
    lighteval vllm \
        "$MODEL_ARGS" \
        "${TASK_ID}|0|0" \
        --use-chat-template \
        --output-dir ./lighteval/outputs \
        --system-prompt "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚" \
        --save-details


# çµæœã®é›†è¨ˆ
echo "ğŸ¦ Aggregating results has started"
uv run --isolated --locked --extra aggregate_results \
    python scripts/aggregate_results.py \
        --model_name "$MODEL_NAME"