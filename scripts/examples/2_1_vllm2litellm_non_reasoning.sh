#!/bin/bash
set -euo pipefail

# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã‚¯ãƒ­ãƒ¼ãƒ³ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼

MODEL_ID="tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5"
MODEL_NAME="hosted_vllm/${MODEL_ID}"
TASK_ID="swallow|japanese_mt_bench"

export OPENAI_API_KEY="{LLM-as-a-Judgeã«ä½¿ã†OpenAI API Key}" 
source scripts/examples/utils.sh


# vLLM serve ã®èµ·å‹•
echo "ğŸ¦ vLLM serve has started"
uv run --isolated --locked --extra vllm \
    vllm serve "$MODEL_ID" \
        --host localhost \
        --port 25819 &
VLLM_SERVER_PID=$!

BASE_URL="http://localhost:25819/v1"
# HuggingFace ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã§ã‚µãƒ¼ãƒ–ã™ã‚‹å ´åˆã«ã¯ "http://localhost:(ãƒãƒ¼ãƒˆç•ªå·)/v1" ã‚’æŒ‡å®šï¼


# vLLMãŒèµ·å‹•ã™ã‚‹ã¾ã§å¾…æ©Ÿ
wait_for_vllm_server "$BASE_URL" "$VLLM_SERVER_PID"


# è©•ä¾¡
echo "ğŸ¦ Evaluation has started"
uv run --isolated --locked --extra lighteval \
    lighteval endpoint litellm \
        "model=$MODEL_NAME,base_url=$BASE_URL" \
        "${TASK_ID}|0|0" \
        --use-chat-template \
        --system-prompt "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚" \
        --output-dir ./lighteval/outputs \
        --save-details


# çµæœã®é›†è¨ˆ
echo "ğŸ¦ Aggregating results has started"
uv run --isolated --locked --extra aggregate_results \
    python scripts/aggregate_results.py \
        --model_name "$MODEL_NAME"