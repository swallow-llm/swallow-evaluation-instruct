[project]
name = "jalm-evaluation-instruct-private"
version = "0.0.1"
description = "beta"
readme = "README.md"
requires-python = "==3.10.14"
dependencies = []


[project.optional-dependencies]
auto_detector = [
  "transformers",
  "sentencepiece",
  "protobuf",
]

aggregate_results = [
    "pandas>=2.2.3",
]

lighteval = [
    "lighteval[math,extended_tasks,litellm]",
    "transformers>=4.51.0", # Set the floor to allow evaluation of the latest models: Llama 4, Phi4-Multimodal, DeepSeek-v3, Qwen3.
    "openai>=1.40.0", # Set the floor to avoid using deprecated names.
]

vllm = [
  "vllm>=0.9", # Set the floor to use --reasoning-parser without setting --enable-reasoning
]


[tool.hatch.metadata]
allow-direct-references = true


[tool.uv.sources]
torch = { index = "pytorch-cu126" }
torchvision = { index = "pytorch-cu126" }

lighteval = { path = "./lighteval", editable = true }


[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"


[tool.uv]
conflicts = [
  [
    { extra = "auto_detector" },
    { extra = "aggregate_results" },
    { extra = "lighteval" },
    { extra = "vllm" },
  ],
]
index-strategy = "unsafe-best-match"


[tool.ruff]
ignore = ["F841", "E741"]
fix = true